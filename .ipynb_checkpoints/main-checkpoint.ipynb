{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV Practical\n",
    "\n",
    "__Overview__\n",
    "\n",
    "Goal of this assignment is to build a deep learning pipeline to predict parameters of a circle ( center (x, y) and radius (r)) embedded in a noisy/occluded image $I_N$. The pipeline delineated in the following sectionscontains 2 key elements:\n",
    "\n",
    "1. The denoising backbone - DnCNN.\n",
    "2. The feature extractor - CDNet.\n",
    "\n",
    "Time taken for modelling the architecture: $\\approx$ 20 minutes and cumulative training time for both the networks: $\\approx$ 2 hours, spread out over a day on an AWS P2 instance.\n",
    "\n",
    "\n",
    "#### Quick links\n",
    "1. [DnCNN]()\n",
    "2. [CDNet]()\n",
    "3. [Results]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The denoising backbone: [DnCNN](https://arxiv.org/pdf/1608.03981.pdf)\n",
    "\n",
    "Inspired from the work by Kai Zhang et. al, the denoising backbone does most of the heavy lifting for CDNet. The denoising network implemented below is a simplified version of the original implementation preserving it's key aspects:\n",
    "1. Batch normalization \n",
    "2. Residual Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as torch_data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "# stats\n",
    "import numpy as np\n",
    "# scale\n",
    "import cv_practical.main as cvp_utils\n",
    "# fancy stuff\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DnCNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    \"\"\" Module implementing a simplified DnCNN\n",
    "    described in https://arxiv.org/pdf/1608.03981.pdf\n",
    "    \"\"\"\n",
    "    def __init__( self, n_channels=64, image_channels=1,\n",
    "        kernel_size=3, init_weights = True ):\n",
    "        super(DnCNN, self).__init__()\n",
    "        \"\"\"Constructor\n",
    "        \n",
    "        Args:\n",
    "            n_channels: Number of filters for each of the convolution\n",
    "                layers.\n",
    "            image_channels: Number of channels of the input image.\n",
    "            kernel_size: size of each convolutional filter.\n",
    "            init_weights: Initializes the convolutional filters orthogonally\n",
    "                preserving the norm.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.name = \"dncnn\"\n",
    "        self._kernel_size = 3\n",
    "        self._n_channels = 64\n",
    "        self._image_channels = image_channels\n",
    "        self._padding = 1\n",
    "\n",
    "\n",
    "        self.conv_b1 = nn.Conv2d(\n",
    "            in_channels=image_channels,\n",
    "            out_channels=self._n_channels,\n",
    "            kernel_size=self._kernel_size, padding=self._padding, bias=True\n",
    "        )\n",
    "        self.nl_b1 = nn.ReLU( inplace = True )\n",
    "\n",
    "        self.conv_b2 = nn.Conv2d(\n",
    "            in_channels = self._n_channels,\n",
    "            out_channels = self._n_channels,\n",
    "            kernel_size = self._kernel_size,\n",
    "            padding = self._padding, bias=False\n",
    "        )\n",
    "        self.bn_2 = nn.BatchNorm2d(self._n_channels, eps = 1e-4, momentum = 0.95)\n",
    "        self.nl_b2 = nn.ReLU( inplace = True )\n",
    "\n",
    "        self.conv_b3 = nn.Conv2d(\n",
    "            in_channels = self._n_channels,\n",
    "            out_channels = self._n_channels,\n",
    "            kernel_size = self._kernel_size,\n",
    "            padding = self._padding, bias=False\n",
    "        )\n",
    "        self.bn_3 = nn.BatchNorm2d(self._n_channels, eps = 1e-4, momentum = 0.95)\n",
    "        self.nl_b3 = nn.ReLU( inplace = True )\n",
    "\n",
    "\n",
    "        self.conv_b4 = nn.Conv2d(\n",
    "            in_channels = self._n_channels,\n",
    "            out_channels = self._n_channels,\n",
    "            kernel_size = self._kernel_size,\n",
    "            padding = self._padding, bias=False\n",
    "        )\n",
    "        self.bn_4 = nn.BatchNorm2d(self._n_channels, eps = 1e-4, momentum = 0.95)\n",
    "        self.nl_b4 = nn.ReLU( inplace = True )\n",
    "\n",
    "        self.conv_b5 = nn.Conv2d(\n",
    "            in_channels = self._n_channels,\n",
    "            out_channels = self._image_channels,\n",
    "            kernel_size = self._kernel_size,\n",
    "            padding = self._padding,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        if init_weights:\n",
    "            self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = x\n",
    "\n",
    "        out = self.conv_b1(x)\n",
    "        out = self.nl_b1(out)\n",
    "\n",
    "        out = self.conv_b2(out)\n",
    "        out = self.bn_2(out)\n",
    "        out = self.nl_b2(out)\n",
    "\n",
    "        out = self.conv_b3(out)\n",
    "        out = self.bn_3(out)\n",
    "        out = self.nl_b3(out)\n",
    "\n",
    "        out = self.conv_b4(out)\n",
    "        out = self.bn_4(out)\n",
    "        out = self.nl_b4(out)\n",
    "\n",
    "        out = self.conv_b5(out)\n",
    "        \n",
    "        return y - out\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initializes weights and biases for the module.\n",
    "        \n",
    "        Weights for the convolutional layers have been initialized \n",
    "        orthogonally and biases to a constant. \n",
    "        \"\"\"\n",
    "        for mdx, module in enumerate(self.modules()):\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                init.orthogonal_(module.weight)\n",
    "                print('initialized layer: {}'.format(mdx), end='\\r')\n",
    "                if module.bias is not None:\n",
    "                    init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                init.constant_(module.weight, 1)\n",
    "                init.constant_(module.bias, 0)\n",
    "        print(\"initialization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sum of Squared Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSE(_Loss):\n",
    "    \"\"\"\n",
    "    sse = 1/2 * nn.MSELoss (reduced by sum)\n",
    "    \"\"\"\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='sum'):\n",
    "        super(SSE, self).__init__(\n",
    "            size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # return torch.sum(torch.pow(input-target,2), (0,1,2,3)).div_(2)\n",
    "        return torch.nn.functional.mse_loss(\n",
    "            input, target, size_average=None, reduce=None, \n",
    "            reduction='sum').div_(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset for training/testing DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNNDataset(torch_data.Dataset):\n",
    "    \"\"\"Dataset for training our simplified DnCNN.\n",
    "    \n",
    "    This dataset inherits from PyTorch torch_data.Dataset \n",
    "    to be used to create batches for PyTorch's dataloader.\n",
    "    Each sample in the dataset is a pair of noisy image (I_n) and\n",
    "    denoised image (I).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, count=1000, noise = 1, \n",
    "                 random_noise = True, debug = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            count: Number of sample image pairs in the dataset.\n",
    "            noise: Max level of gaussian additive noise.\n",
    "            random_noise: If set to True, additive gausian noise \n",
    "                is multiplied by a random constant sampled from a \n",
    "                uniform distruibution in range (0, noise)\n",
    "            debug: If set to True, writes image pairs to disk.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._deubg = debug\n",
    "        self._count = count\n",
    "        self._noise = noise\n",
    "        self._random_noise = random_noise\n",
    "        self._circle_images = []\n",
    "\n",
    "        self.create_dataset()\n",
    "\n",
    "    def create_dataset(self):\n",
    "        for i in range(self._count):\n",
    "            noise = np.random.uniform(0, self._noise) \\\n",
    "                if self._random_noise else self._noise\n",
    "\n",
    "            params, img, img_noise = cvp_utils.noisy_circle(200, 50, noise)\n",
    "            # normalize.\n",
    "            img = (img - np.min(img))/(np.max(img) - np.min(img))\n",
    "            img_noise = (img_noise - np.min(img_noise))/(np.max(img_noise) - np.min(img_noise))\n",
    "            # add a channel axis to conform to\n",
    "            # PyTorch's tensor specifications.\n",
    "            self._circle_images.append(\n",
    "                [\n",
    "                    np.expand_dims(img_noise, axis=0),\n",
    "                    np.expand_dims(img, axis=0)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._circle_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._circle_images[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dncnn(model, optimizer, criterion, device, dataloader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    tbar = tqdm(dataloader)\n",
    "    num_samples = len(dataloader)\n",
    "    for i, sample in enumerate(tbar):\n",
    "        image, target = sample[0].float(), sample[1].float()\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        tbar.set_description('Train loss:  %.3f' % (train_loss / (i + 1)))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "\n",
    "def validate_dncnn(model, criterion, device, dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    tbar = tqdm(dataloader)\n",
    "    num_samples = len(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(tbar):\n",
    "            image, target = sample[0].float(), sample[1].float()\n",
    "            image, target = image.to(device), target.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            tbar.set_description('Val loss:    %.3f' % (train_loss / (i + 1)))\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "\n",
    "def test_dncnn(model, device, dataloader, debug = False):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    tbar = tqdm(dataloader)\n",
    "    num_samples = len(dataloader)\n",
    "    outputs = []\n",
    "    ious = []\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(tbar):\n",
    "\n",
    "            image = sample[0].float()\n",
    "            image = image.to(device)\n",
    "            outputs.append([sample[0], model(image), sample[1]])\n",
    "    if debug:\n",
    "        for bdx, b in enumerate(outputs):\n",
    "            for idx , i in enumerate(zip(b[0], b[1], b[2])):\n",
    "                img = i[0][0].cpu().numpy()\n",
    "                pred_params = i[1][0].cpu().numpy()\n",
    "                target_params = i[2][0].cpu().numpy()\n",
    "\n",
    "                plt.imsave(\"./results/{}.png\".format(idx), img)\n",
    "                plt.imsave(\"./results/{}_pred.png\".format(idx), pred_params)\n",
    "                plt.imsave(\"./results/{}_targ.png\".format(idx), target_params)\n",
    "\n",
    "                \n",
    "def total_parameters(model):\n",
    "    \"\"\"Get number parameters in a network.\n",
    "    \n",
    "    Args:\n",
    "        model: A PyTorch nn.Module object.\n",
    "    \n",
    "    Returns:\n",
    "        num_parameters (int): total parameters in a network.\n",
    "    \n",
    "    \"\"\"\n",
    "    model_parameters = filter(\n",
    "        lambda p: p.requires_grad, model.parameters())\n",
    "    \n",
    "    return sum([np.prod(p.size()) for p in model_parameters])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXECUTE THIS AT YOUR OWN RISK. \n",
    "\n",
    "\n",
    "Jupyter notebooks occupy a healthy amount of RAM and training the model via this notebook might crash the PC. use the script `denoise_trainer.py` instead to train on a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # number of trainig epochs\n",
    "# explicitly set device to cuda:0 when pipeline is funtional. \n",
    "# setting device to cpu gives more elaborate error trace.\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_dataset = DnCNNDataset(\n",
    "    count = 10000,\n",
    "    random_noise = False,\n",
    "    noise = 2,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "val_dataset = DnCNNDataset(\n",
    "    count = 1000,\n",
    "    noise = 2,\n",
    "    random_noise = False,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "test_dataset = DnCNNDataset(\n",
    "    count = 1000,\n",
    "    noise = 2,`\n",
    "    random_noise = False,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "# PyTorch data loaders.\n",
    "train_dataloader = torch_data.DataLoader(train_dataset, num_workers=0, batch_size=32)\n",
    "val_dataloader = torch_data.DataLoader(val_dataset, num_workers=0, batch_size=32)\n",
    "test_dataloader = torch_data.DataLoader(test_dataset, num_workers=0, batch_size=32)\n",
    "\n",
    "model = DnCNN()\n",
    "print(\"total parameters: {}\".format(total_parameters(model)))\n",
    "model.to(device)\n",
    "\n",
    "# Adam optimizer with initial learning rate of 5e-3\n",
    "# and momentum of 10e-2\n",
    "optimizer = torch.optim.Adam(\n",
    "    lr=0.005, weight_decay=1e-3, params=model.parameters()\n",
    ")\n",
    "\n",
    "# Reducaes LR when validation loss plateaus for 10 epochs.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "# cost function\n",
    "criterion = SSE()\n",
    "\n",
    "# store train and validation loss in this array\n",
    "# to be used later for graphing and facilitate \n",
    "# hyper-parameter search.\n",
    "train_meta = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_dncnn(model, optimizer, criterion, device, train_dataloader)\n",
    "    val_loss = validate_dncnn(model, criterion, device, val_dataloader)\n",
    "    test_score = test_dncnn(model, device, test_dataloader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(epoch, train_loss, val_loss, test_score)\n",
    "\n",
    "    train_meta.append(\n",
    "        [train_loss, val_loss, test_score]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    model_save_str = './results/models/{}-{}.{}'.format(\n",
    "        model.name, epoch, \"pth\"\n",
    "    )\n",
    "\n",
    "    torch.save( state,model_save_str )\n",
    "    np.save(\"train_meta_denoiser\", np.array(train_meta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model converged easily with the train a test loss trends shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2792305b940>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuUHOdZ5/HvU9Xdc9NldBnZju5O5MSOjWNHOM4FCImztkOwFzCLveEQ2BAvh2QDIWc5ZllCNsDhtmwSWC+LCQESwE5IQiK8BmPsBJIQB8sYG0u2bFm+aCxLGknWdS59qWf/eKtHrVF3T0vq0ai6fp9z5nRXdXX32yr7128/9dZb5u6IiEhviea7ASIi0n0KdxGRHqRwFxHpQQp3EZEepHAXEelBCncRkR6kcBcR6UEKdxGRHqRwFxHpQYX5euPly5f7unXr5uvtRUQy6eGHH97n7iOzbTdv4b5u3To2b948X28vIpJJZvZ8J9upLCMi0oMU7iIiPUjhLiLSgxTuIiI9SOEuItKDZg13M/u0me01s8dbPG5m9rtmtt3MHjOzK7vfTBERORWd9Nz/BLiuzePXAxvSv1uB3z/zZomIyJmYNdzd/R+BA202uRH4jAcPAsNmdkG3GjjTQ88d4Hf+bhuVWtJ6o6QG//JZqFXnqhkiIue0btTcVwI7G5ZH03UnMbNbzWyzmW0eGxs7rTd75IWX+b0HtlOutgn30Ydg0wfg+W+e1nuIiGRdN8LdmqxretVtd7/D3Te6+8aRkVnPnm0qjkKTq0mbC3tXJsJtdfK03kNEJOu6Ee6jwOqG5VXAri68blOFKHyXVNuVZWqV9LY8V80QETmndSPcNwE/lo6auRo45O4vdeF1m4rTcK+167knCncRybdZJw4zszuBtwLLzWwU+GWgCODu/xe4B3gnsB0YB35irhoLUIzTnnu7cK+Hug6oikhOzRru7n7LLI878P6utWgW9Zp72567yjIiknOZO0O1XnNvOxRS4S4iOZe5cO+o5j5dlqmchRaJiJx7MhfundXc1XMXkXzLXLh3VHOvj5ZJ1HMXkXzKXLh3VnNXWUZE8i174R53UnNXWUZE8i1z4V4/oKqau4hIa5kL90JH49xVlhGRfMtcuMed1NyT9MxU9dxFJKcyF+7Fjmru6rmLSL5lLtw7q7kr3EUk3zIX7vWae7XWLtxVlhGRfMtcuB/vuXcyzl3hLiL5lLlwV81dRGR2mQv3jmru9dEymn5ARHIqc+HeWc1dPXcRybfMhfvxKX9VcxcRaSVz4d7ZlL8aLSMi+Za5cNfFOkREZpe5cK/X3CuquYuItJS5cO+o5q65ZUQk5zIX7gVNPyAiMqvMhXsUGZF1WnNXz11E8ilz4Q6h7t6+5q6yjIjkWzbDPbYOx7mrLCMi+ZTJcI8j66zmrukHRCSnMhnuhcja19wbR8t4m+1ERHpUJsM9nrXm3lBrrwe9iEiOZDLci+1q7u4h3Av9YVkHVUUkhzIZ7m1r7kkt3BYHw63CXURyqKNwN7PrzGybmW03s9uaPL7GzL5qZo+Y2WNm9s7uN/W4QmStp/yth3lpKF1WWUZE8mfWcDezGLgduB64BLjFzC6Zsdl/Bz7v7lcANwP/p9sNbRS3O6B6Urir5y4i+dNJz/0qYLu773D3MnAXcOOMbRxYlN5fDOzqXhNPVoyj1tdQrR9AVVlGRHKs0ME2K4GdDcujwBtmbPNR4O/M7L8AQ8A1XWldC6fWc9dYdxHJn0567tZk3cxkvQX4E3dfBbwT+KyZnfTaZnarmW02s81jY2On3tpUIbLWQyGnw33BicsiIjnSSbiPAqsblldxctnlvcDnAdz9W0A/sHzmC7n7He6+0d03joyMnF6Lma3nnpZlSmlZRmepikgOdRLuDwEbzGy9mZUIB0w3zdjmBeDtAGZ2MSHcT79rPotCu5p7vac+XXNXuItI/swa7u5eBT4A3As8QRgVs8XMPmZmN6SbfRh4n5k9CtwJ/Lj73J3333b6AZVlREQ6OqCKu98D3DNj3Uca7m8F3tzdprUWR8Z4udVJTDPKMgp3EcmhTJ6h2lnPXaNlRCS/shnucdR6+gGVZUREMhruUZuJw2ozT2JSz11E8ieT4R53NLeMwl1E8iuT4V5UWUZEpK1Mhnvbk5g0t4yISDbDPUw/MMtJTBotIyI5lslwP6WJwzT9gIjkUCbDvX3NPQ1zlWVEJMcyGe7te+5puKssIyI5lslw76jmXuiDqKCeu4jkUibDvf1ombSnHhUhLincRSSXMhnu9ekHmk48WS/DxMUQ8LpAtojkUDbDPQoXh2raea+VQ6ibhYBXz11EciiT4R6n4d607l6rhFAHlWVEJLcyGe71nnvTuvsJ4V7UaBkRyaVshnscmt10rHutHHrsoJ67iORWNsO9Xc89qYSaO6jmLiK5lclwr9fcq7PW3IvHJxITEcmRTIZ7MU7DXWUZEZGmMhnucRSaPfsB1ZIOqIpILmUy3AsdD4VUzV1E8imT4R63HQrZUJaJFO4ikk+ZDPe2Nfek2jBapqTpB0QklzIZ7u1r7mWVZUQk9zIZ7u1r7hotIyKSyXBvX3OvarSMiOReJsO9MOs493q462IdIpJP2Qz3WWvuDWUZXSBbRHIok+Hedsrfk0bLKNxFJH8yGe7tp/zVaBkRkY7C3cyuM7NtZrbdzG5rsc1/MLOtZrbFzP6iu8080ew19xmjZZpdjk9EpIcVZtvAzGLgduAdwCjwkJltcvetDdtsAH4BeLO7v2xmK+aqwTBbzb1htEy9PJM0rBMRyYFOeu5XAdvdfYe7l4G7gBtnbPM+4HZ3fxnA3fd2t5knan+ZvRllmfo6EZEc6STcVwI7G5ZH03WNLgIuMrNvmtmDZnZdsxcys1vNbLOZbR4bGzu9FtNJzb2hLAM6qCoiudNJuFuTdTNTtQBsAN4K3AJ8ysyGT3qS+x3uvtHdN46MjJxqW4+/Wauae1ILTYtm9twV7iKSL52E+yiwumF5FbCryTZfcfeKuz8LbCOE/Zyo19yrtRnhXi+/NJ6h2rheRCQnOgn3h4ANZrbezErAzcCmGdt8GfheADNbTijT7OhmQxsdn35gRs19OtzrZRnV3EUkn2YNd3evAh8A7gWeAD7v7lvM7GNmdkO62b3AfjPbCnwV+K/uvn+uGt1yyt/69L4n9dxVlhGRfJl1KCSAu98D3DNj3Uca7jvwc+nfnGs5cdhJZZn6UEiFu4jkS0bPUA3NrrSsuc8cLaOyjIjkSybDvWXNPUnLMhotIyI5l8lwr49zP7nmrtEyIiKQ0XCPIiOydjX3hgtkN64XEcmJTIY7hLr7yTX3VqNldJFsEcmXzIZ7HFmbce6aW0ZE8i2z4V6Ircn0A+mBU42WEZGcy264R9a65q7RMiKSc5kN97hpzb3ec1dZRkTyLbPhXmhac29RltEZqiKSM9kN92Y195bj3BXuIpIv2Q33yJpM+auyjIgIZDjc42YHVDVaRkQEyHC4F6KIaqtx7tGMC2SrLCMiOZPdcI+bDYWcUZaJIrBYPXcRyZ3shntkbYZClo6vi0vquYtI7mQ23JvW3GeOlgGFu4jkUmbDvRA3q7mnIR41hntRZRkRyZ3shnur0TJRIdTa6xTuIpJDmQ33uGnNvXxirx3ScFdZRkTyJbPh3nzisMqJB1MhLGv6ARHJmeyGexw1mX6gcuLBVEgPqKosIyL5kt1wb3WxjpPCXWUZEcmfzIZ73GpumZnhHumAqojkT2bDvRC1uBJTs5q7eu4ikjPZDfc4an4Sk0bLiIhkONwja34Skw6oiohkN9xb19xVlhERyWy4N625Nx0tU1DPXURyJ7vh3rTmrrKMiAh0GO5mdp2ZbTOz7WZ2W5vtbjIzN7ON3Wtic2HK3xk1d42WEREBOgh3M4uB24HrgUuAW8zskibbLQQ+CHy7241spuWUv81Gy2j6ARHJmU567lcB2919h7uXgbuAG5ts9yvAbwGTXWxfS/XpB9wbAl5lGRERoLNwXwnsbFgeTddNM7MrgNXufncX29ZWITIATui8NxstE2mcu4jkTyfhbk3WTUeqmUXAx4EPz/pCZrea2WYz2zw2NtZ5K5uI03A/oe7etOeu6QdEJH86CfdRYHXD8ipgV8PyQuBS4Gtm9hxwNbCp2UFVd7/D3Te6+8aRkZHTbzXHe+4n1N2bDoVMyzI+oz4vItLDOgn3h4ANZrbezErAzcCm+oPufsjdl7v7OndfBzwI3ODum+ekxalCHJp+wlj3VqNlAJLaXDZHROScMmu4u3sV+ABwL/AE8Hl332JmHzOzG+a6ga0077lXmo+WAZVmRCRXCp1s5O73APfMWPeRFtu+9cybNbt6zb16Qs29xXzu9ccYPBtNExGZd9k9Q7Ue7jN77s1q7vXHRERyIrvhntbcp8sySQ281qTmrrKMiORPdsN9Zs+93jNv1XPXWaoikiOZDfeTau718D7pgKrKMiKSP5kN99Y995lnqKbHjFWWEZEcyW64z6y518O75QFVhbuI5Ed2w33m9AOz1dxVlhGRHMlsuMczT2Ka7rlrtIyISGbD/ZRHy6jnLiI5kt1wP2mcu0bLiIjUZTbcT5ryt2VZRqNlRCR/shfuT/4/uOvdFCz02GuzlmX6wm31rFwgSkTknJC9cD/yEjx5N/1T4WIfs9bcB5aE24mXz1IDRUTmX/bCfcl6AAaOPA90MFpmcBlgcOzMrvwkIpIl2Qv3pRcC0HfkBaCTce4FGFyqcBeRXMleuC9eDVGBvsPPAQ0998lD4ba04OTnDK2Ao3vPTvtERM4B2Qv3uADDayil4T5dc9+3DSyeLtucYGg5HNt39tooIjLPshfuAEsvpHhoRs19bFso2RRKJ2+/YAUcU89dRPIjm+G+ZD3xwWcBPz7l79g2GHl18+2HRuCoau4ikh/ZDPelFxKVj7CEI6EsUy3DgR0w8prm2w+NQPkIVCbObjtFROZJRsM91NXX2Z5QljnwTLjEXque+4IV4VYjZkQkJzIa7mE45FrbQ6XmMPZkWN+uLAMKdxHJjWyG+/BaHGOt7aGWJDD2FGCwbEPz7YfSnrvq7iKSE9kM92I/LFrJ2mhPqLmPPQnDa6A02Hz7oeXhViNmRCQnshnugC1dn/bcHfY91fpgKqjmLiK5k9lwZ+l61tkeqtUq7HsaRi5qvW1xAEoLVZYRkdzIcLhfyDI7zPKjT0Ftqn3PHdKzVBXuIpIPmQ53gFe+/PWwvLzFSJk6naUqIjmS3XBP55DZcPAbYbldWQbCcEjNLyMiOZHdcE9PZFo5sQ0WXgD9i9tvPzSimSFFJDeyG+59CzlAGuitTl5qtGAFjO+HWnVu2yUicg7oKNzN7Doz22Zm283stiaP/5yZbTWzx8zsfjNb2/2mnmw0uiDcme1gKqRnqTpMHJjTNomInAtmDXczi4HbgeuBS4BbzOySGZs9Amx09+8AvgD8Vrcb2syuergvn6XeDsenIFBpRkRyoJOe+1XAdnff4e5l4C7gxsYN3P2r7j6eLj4IrOpuM5vbHZ0f7nTcc0fDIUUkFzoJ95XAzobl0XRdK+8F/qbZA2Z2q5ltNrPNY2NnHrKPFq9gtLQezr9s9o11lqqI5Egn4W5N1nnTDc1+FNgI/Hazx939Dnff6O4bR0ZGOm9lC0/1XcJHV/4h9C+afWP13EUkRwodbDMKrG5YXgXsmrmRmV0D/CLwPe4+1Z3mtVeILEz524n+xRCXVHMXkVzopOf+ELDBzNabWQm4GdjUuIGZXQH8AXCDu5+19IwjO34N1dmYpScyqecuIr1v1nB39yrwAeBe4Ang8+6+xcw+ZmY3pJv9NrAA+Esz+1cz29Ti5bqqEEVUk6TzJ2h+GRHJiU7KMrj7PcA9M9Z9pOH+NV1uV0cKsVGpnUq4r1BZRkRyIbtnqBLKMh3X3CGdPEzzy4hI78t0uBdOpeYOaVlmL/gpPEdEJIMyHe5xFIXL7HVqaAXUyjB1eO4aJSJyDsh0uBdjCxfI7tT0FAQ6qCoivS3T4R5HRvWUau71E5l0UFVEelumw70Q2amXZeDsDYesTMKLD5+d9xIRaZDtcI+jUzuguugVYBE8ehfUKnPXsLp/+j34w7fB038/9+8lItIg2+Ee2amdxDS4FK79ddh2D3zxJ+f2wh3u8Njnwv27fxamjs7de4mIzNDRSUznqlOuuQNc/VPgNbj3v4Ve/OveDbsegd2PweU3w2u+rzuN2/0Y7H8aLv+P8Oid8MCvwPW/2Z3XFhGZRabD/ZRr7nVvfD8kNbjvl2DLl8K60kJ45gH4qa/D0gvPvHH/9gWICnDtr0FpCL79B3DpD8Hqq878tUVEZpHtssyp1twbvfmD8OP3wHvuhtt2wk9/C6IYvvi+M6/HJwk8/iV45dtDKeiaX4ZFK+HLPw1P3wfl8dlfQ0TkDGQ73KNTnFtmpnVvhvXfFeaDH14N3/9JeHEzfO03On8N9zAi5pkHjq/b+SAcHoXLbgrLfQvhxv8Nh3fBn98Ev7Ue/uJmjbcXkTmT6bLMKU3524nX/kAY2fL134FCHwwsCb35wWWw9JWhXFPog/EDcHQPPPuP8MhnYe/W8PxrPgpv+VAoyRQG4NXvPP7ar/xe+Pln4Plvwvb74aFPwd/eBjf9UffaLyKSynS412vu7o5ZswtGnYbrfxNeehS++mvNH7cIvOHXwsrXw7s+Ac99A/7+ozDxMmz5K3j19dC34MTnFgfgVdeEv/7F8LVfDwdxN7yjO20XEUllO9zjUFVKHOIuZTt9C+A//0OYfyaphfr70T1wYAcceAaqU+FkqAUjMHIxrEgvzn3lj4XnfvOTYfmyH27/Pm/5EDz+Rbj75+D9D4aDriIiXZLpcI+jkOiVWkIcxd174SgOJZm6RRfAK143+3Pe9QkYXA47vgavenv77Qt9ocb/x9eHHvy/+9UzbraISF3mD6gC3a27nwkzePsvwfvuD+E9m7VvgivfA9+6HZ64e+7bJyK5kelwr/fcT2us+7niHR+DCy6Hz70b7vvluT1rVkRyI9PhXkxr7udMz/10DAzDT/wtbPxP8M1PwGduDPV9EZEzkOlwr/fcD46X2X1okl0HJ/AsXmWp2A/v+jj8wB/Arn+B39sIX36/Ql5ETlumD6iWCuG76W2/8w/T6y5cPsS1l57P9Zeez2UrF3dviOTZcPnNcOFb4RufgM2fDnPSXPFu+J7bYPHK+W6diGSIzVdPd+PGjb558+Yzeo2D42X+4p9foBhFDPUVmKrWeODJvfzTM/upJc5F5y3glqvW8INXrGLxYLFLLT9LDr8E3/g4PPzHgMFV74M3fRAWnjffLROReWRmD7v7xlm3y3K4t3JwvMzfPL6bu/75BR4dPURfIeLa157PTa9fxZtftXy6nJMJB18I0yE8eidYDJfcAN/5k7ByYzihyqIwSidLv1BE5LTlOtwbPf7iIT730E6+8q8vcniyygWL+7n2tefzva9ZwRvWL6W/2MXx8XNp/zNhyoJH/hymDp34mMXh7NfiAAyvhfXfDRd+D6y+OtTzRaRnKNxnmKzUuP+JvfzVI6N8/el9TFUTBksx371hhGsvPY+3vfq8bJRuysdg61fg8IvghLnpaxWoTEDlGOx9AkY3h/V9i+DSHwxz1g+vhSf/Ojz34M5wktWrr4c1b4LKeJg2oTKenn27IpyUJSLnHIV7GxPlGt/asY/7n9jLfVv3sPfIFIXIuHz1MBvXLeE71y7lO1YtZmRhX7YOyNZNHYHnvglbvxzCvNIwxfCyDWECtOe+fuL6RhbDwgvgvEvg/MvCNAtxIUzHUJmAfU/BnsdDyegtH4IrfvTsfC4RUbh3Kkmcx148xH1bd/PgjgM8NnqQSnp1p8UDRS46bwGvHFnAmmWDrF06xNplg6xfPsRQX0YGGk0dCQF/ZHeYpXLFxaE+X5mAHf8QrhjVvxgGloazao/tDQdzD74Ae7bAvm2QzDixKi7ByGsAh92Pww996vj0xiIypxTup2myUuOx0UNs3XWIp/Ye5andR3h23zH2HyufsN2KhX2sXjrI8gUlli/o4/xF/eELYNkQ65YNMjxYmqdP0GXVKTjwLODhylJxERatCj35ygT82U1h/vof+bNQ5hGROaVw77IjkxVeODDO8/vHeXbfMZ7bd4wXD06w7+gU+46WOTAj/IcHi6xfPsTapYMsX9DH0gUlRhb0seG8hVx03gIGSxnp+c9m8nA4q3bPFrj4+0OvPi6Gcs7F3w8Lz5/vFor0FIX7WTZZqbHzwDjP7R/nuX3HeHZ/+AJ44cA4+4+WmajUprc1g1VLBlixsJ9lQyWWLehj1ZIB1iwdZM3SQUYW9rF0qNTRSJ5DExX6CtH8jvoZPwBfuhX2bw8lnMoEjO8DDNa8Ec6/NIR+oS9MbTywJPwtfEWYMrl/8fy1XSRjuhruZnYd8EkgBj7l7r8x4/E+4DPA64H9wI+4+3PtXrPXwn024+Uquw9N8tSeo2zbfYRnxo6y7+gU+4+WGTs6dVLPH6C/GLFksMTigeL07aKBAgv6irx0aILHdx1i54EJ+osR37VhhHdcch5XrlnC4oEiiweK02fwzou9T4Ra/xN/DYdGoVYOJR6vnbxtPeSXbYDlG2B4DRT6w5dBoQ+Kg2GYZ2Hg+JDP0x3Nc3RvOJjsHkYF1UcHDSzRuQKSCV0LdzOLgaeAdwCjwEPALe6+tWGbnwa+w91/ysxuBn7A3X+k3evmLdxnc2yqys6Xx3lh/zgHjpU5MF7m5WNlDo5XeHm8wsHxMocnKxyeqHJ4ssLyBX1ctnIxl7xiEXsOT3Lf1j28dGjyhNfsK0QMlGIGi/H0xcRriVOIjWVD4VjB8GCJgVJEfyGmvxgzUIoZSG/7ixF9hZNvS4WIYhxhwI59R9ny4mG27TnCyMI+Ll81zOWrh7lgcT99hejk0UaVCZg4GIZeHtoZLlG490kYezL0/MtHO/sHi0sNYd8PpQXp31D4QoiLEBWPl4kgXOt2z+PNXy8qwNBIKCMtXhWOKyw8LxxoHlyW/toYhv5hKA2GLwdP0r/0/6Hp5fQLrDAQti0O6otDuqab4f5G4KPufm26/AsA7v7rDdvcm27zLTMrALuBEW/z4gr37nJ3tuw6zDNjRzk0UeHgeIWjU1UmKzUmyrX0giYRcQSVmrP/WJl9R6Y4OF5mspqE7So1TrdKt3J4gAPHTi4/9Rdi+ooRpTh8IZQKEYXIKMQRxdiII6MYRcSRUYhgmR9gRbKPPqtSsgr9lOnzMn1M0edTlBruF71MKZmk5JOUkglKtQlKyQSxV4i9SiEpE3mVyCtEXmP/0KvYOfwGnl98FZV4gMHyAQYr+xmqHGCoEu4vKI+xcGoPQ5O7KdZaDBU9DUlUwuP0LyrgUTH8FQfwwhBJcRADzKtY+iVhpDsjLuHFQbw4CHFf+NUSp8/tGybpH8aLQxBFWBSDhQPfFhfDge+oCIUSFhXALLwPYJEBERbVz3SOp1/DokK4NQP3sH2xD0qDWGnB8ctNehLup6+NeyjN1crp5SjTs6fL46FUd2xf+IKP0udMHAxfuLv/LXzhn/dauOB1YVRXaej4L7a4ePyA/vSZ2Q3vmyOdhnsnR/VWAjsblkeBN7Taxt2rZnYIWAbs66y5cqbMjEtXLubSladfv3Z3yrWEyXLCeKXKVCVhslpjspIwVakxWQ23lZpTqSVUE2fN0kFec8FCFvUXqdYSnt57lMdGD7L/WJnJco3xco1yLaFSS5iqJlRqTrWW3iYJtcSppvcnq86hZBHbagtJkuOP19xJEtJlSDy8RuJMP+4efpW0nf35CKHbAUAVWJT+rW/2r8EAUyzhKEvsKMN2hMUcY7EdY4AyCUaNiBB74BierkvSdf2UGWKSAStTokKJKiUqFKhRtBpFqgwwxSATDFg46zg8PyJxm37toh1mkCkGmKLPKsQkFKgyyBRDNnXa+7vbqh4R4UR2aj2Eqkc8Z6/gEAu5aOedLOTTHT83wahQoEIh/XepEVNL909MjZgKBabSf/mYhH6m6GeKmCScB0hElZgyRaYohedZ2A8RnnYmyhSoUSOiRkySPl4Xk1DwKhEJVStMv3dQ//cwEoyEiD2v/zBXvuvWU/p3OlWdhHuzr8WZe6+TbTCzW4FbAdasWdPBW8vZZGb0FWL6CjGLOfWzdQtxxMUXLOLiCxbNQes64+64Qy0N++PrQwcvjowo7b1CepKvhy+FJH1OpZZQriXTZawkCa+XTH+JHN82Sd/PqV9XwKe/dML68PzE619SYX2Svl7N4ZDDy+7Tbak1bFN/jfp7TL9m+tGipEKpcphCbQJLakCCJVUsqaS31fBrIKkQJdXp/yndPX0xD8/xEDuW1DCScMJaeiH4+q+5QjJFoTZBIZlI/+3CF5B5gnktvL5FVK1IzYokZuBgOFUrMV5cwnhxCeWof/r1y9EAe/vXUrVw5TLzhCVTu1g29TzFZIpiMkUhmSJKf41FXgvvnL5n7NXwSy2pkFhEzQokxIATNTxeSMoUkjKJxZSjASpRCHHDMXciamEbLxN7hSj993CMipUoR/0kxCGavXa8HYQdU7OYxArUiEKsp78Wg+nfYERew3D6F839BICdhPsosLpheRWwq8U2o2lZZjFwYOYLufsdwB0QyjKn02CRdswMM4gwOh9AlK+f9ee+y+a7AT2hk+EUDwEbzGy9mZWAm4FNM7bZBLwnvX8T8EC7eruIiMytWXvuaQ39A8C9hKGQn3b3LWb2MWCzu28C/gj4rJltJ/TYb57LRouISHsdnSbp7vcA98xY95GG+5PAD3e3aSIicroyfQ1VERFpTuEuItKDFO4iIj1I4S4i0oMU7iIiPWjepvw1szHg+dN8+nLyObVBHj93Hj8z5PNz5/Ezw6l/7rXuPjLbRvMW7mfCzDZ3MnFOr8nj587jZ4Z8fu48fmaYu8+tsoyISA9SuIuI9KCshvsd892AeZLHz53Hzwz5/Nx5/MwwR587kzV3ERFpL6s9dxERaSNz4W5m15nZNjPbbma3zXd75oKZrTazr5rZE2a2xcx+Jl2/1MzuM7On09sl893WbjOz2MweMbO70+X1ZvZpDlhzAAADN0lEQVTt9DN/Lp12uqeY2bCZfcHMnkz3+Rtzsq8/lP73/biZ3Wlm/b22v83s02a218web1jXdN9a8Ltptj1mZleeyXtnKtzTi3XfDlwPXALcYmaXzG+r5kQV+LC7XwxcDbw//Zy3Afe7+wbg/nS51/wM8ETD8m8CH08/88vAe+elVXPrk8DfuvtrgMsJn7+n97WZrQQ+CGx090sJ04nfTO/t7z8BrpuxrtW+vR7YkP7dCvz+mbxxpsIduArY7u473L0M3AXcOM9t6jp3f8nd/yW9f4TwP/tKwmf903SzPwX+/fy0cG6Y2Srg+4BPpcsGvA34QrpJL37mRcB3E66JgLuX3f0gPb6vUwVgIL162yDwEj22v939Hzn5qnSt9u2NwGc8eBAYNrMLTve9sxbuzS7WvXKe2nJWmNk64Arg28B57v4ShC8AYMX8tWxOfAL4eSBJl5cBB929mi734v6+EBgD/jgtR33KzIbo8X3t7i8C/xN4gRDqh4CH6f39Da33bVfzLWvh3tGFuHuFmS0Avgj8rLsfnu/2zCUzexew190fblzdZNNe298F4Erg9939CuAYPVaCaSatM98IrAdeAQwRyhIz9dr+bqer/71nLdw7uVh3TzCzIiHY/9zdv5Su3lP/mZbe7p2v9s2BNwM3mNlzhHLb2wg9+eH0Zzv05v4eBUbd/dvp8hcIYd/L+xrgGuBZdx9z9wrwJeBN9P7+htb7tqv5lrVw7+Ri3ZmX1pr/CHjC3f9Xw0ONFyJ/D/CVs922ueLuv+Duq9x9HWG/PuDu7wa+SrjoOvTYZwZw993ATjN7dbrq7cBWenhfp14ArjazwfS/9/rn7un9nWq1bzcBP5aOmrkaOFQv35wWd8/UH/BO4CngGeAX57s9c/QZ30L4OfYY8K/p3zsJNej7gafT26Xz3dY5+vxvBe5O718I/DOwHfhLoG++2zcHn/d1wOZ0f38ZWJKHfQ38D+BJ4HHgs0Bfr+1v4E7CMYUKoWf+3lb7llCWuT3Ntn8jjCQ67ffWGaoiIj0oa2UZERHpgMJdRKQHKdxFRHqQwl1EpAcp3EVEepDCXUSkByncRUR6kMJdRKQH/X/XNhXpgGxZ7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_max = lambda a: (a - np.min(a))/(np.max(a) - np.min(a))\n",
    "\n",
    "train_meta = np.load(\"train_meta_denoiser.npy\")\n",
    "\n",
    "plt.plot(min_max(train_meta[:, 0]))\n",
    "plt.plot(min_max(train_meta[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the resluts of denoising the image using trained DnCNN at 70th epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|input|prediction|target|\n",
    "|------|------|------|\n",
    "|![](./results/images/0.png)|![](./results/images/0_pred.png)|![](./results/images/0_targ.png)|\n",
    "|![](./results/images/1.png)|![](./results/images/1_pred.png)|![](./results/images/1_targ.png)|\n",
    "|![](./results/images/2.png)|![](./results/images/2_pred.png)|![](./results/images/2_targ.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting parameters of a noisy circle using CDNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using this denoiser as a backbone for our CDNet. A noisy image will be first input to a trained DnCNN and the resulting putput will be used to predict the circle parameters using a regression based loss - MSELoss in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CDNet Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_planes,\n",
    "        bbone,\n",
    "        bbone_weights = None\n",
    "    ):\n",
    "        super(CDNet, self).__init__()\n",
    "\n",
    "        self.denoiser = bbone\n",
    "\n",
    "        if bbone_weights is not None:\n",
    "            self._init_bbone(bbone_weights)\n",
    "\n",
    "        self.fc1   = nn.Linear(33*33, 20*20)\n",
    "        self.fc2   = nn.Linear(20*20, 20)\n",
    "        self.fc3   = nn.Linear(20, 3)\n",
    "\n",
    "        self.name = \"cdnet\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        b_dx, c_dx, w, h = x.size()\n",
    "\n",
    "        out = self.denoiser(x)\n",
    "\n",
    "        out = F.avg_pool2d(out, 3, stride=3)\n",
    "        out = F.avg_pool2d(out, 2, stride=2)\n",
    "\n",
    "        out = out.view(b_dx, -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _init_bbone(self, bbone_weights):\n",
    "        weights = torch.load(bbone_weights)\n",
    "        self.denoiser.load_state_dict(weights['model'])\n",
    "\n",
    "        # freeze layers for the denoiser\n",
    "        for module in self.denoiser.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                print('layer frozen.', end='\\r')\n",
    "                for parameters in module.parameters():\n",
    "                    parameters.requires_grad = False\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset for training/testing CDNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIRCLEDataset(torch_data.Dataset):\n",
    "    \"\"\"Dataset for training CDNet.\n",
    "    \n",
    "    This dataset inherits from PyTorch torch_data.Dataset \n",
    "    to be used to create batches for PyTorch's dataloader.\n",
    "    Each sample in the dataset is a pair of noisy image (I_n) and\n",
    "    the parameters of the circle in that image (x, y, r).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, count=1000, noise = 1, \n",
    "                 random_noise = True, debug = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            count: Number of sample image pairs in the dataset.\n",
    "            noise: Max level of gaussian additive noise.\n",
    "            random_noise: If set to True, additive gausian noise \n",
    "                is multiplied by a random constant sampled from a \n",
    "                uniform distruibution in range (0, noise)\n",
    "            debug: If set to True, writes image pairs to disk.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._deubg = debug\n",
    "        self._count = count\n",
    "        self._noise = noise\n",
    "        self._random_noise = random_noise\n",
    "        self._circle_images, self._circle_params = [], []\n",
    "\n",
    "        self.create_dataset()\n",
    "\n",
    "    def create_dataset(self):\n",
    "        for i in range(self._count):\n",
    "            noise = np.random.uniform(0, self._noise) if self._random_noise else self._noise\n",
    "\n",
    "            params, img, img_noise = cvp_utils.noisy_circle(200, 50, noise)\n",
    "            # normalize\n",
    "            img = (img - np.min(img))/(np.max(img) - np.min(img))\n",
    "            # add a channel axis to conform to\n",
    "            # PyTorch's tensor specifications.\n",
    "            self._circle_images.append(\n",
    "                np.expand_dims(img, axis=0)\n",
    "            )\n",
    "            \n",
    "            # normalize params.\n",
    "            self._circle_params.append((np.asarray([\n",
    "                    (params[0]-100)/100.0,\n",
    "                    (params[1]-100)/100.0,\n",
    "                    (params[2]-10)/40.0\n",
    "                ], dtype = np.float32)\n",
    "            ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._circle_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [\n",
    "            self._circle_images[idx], self._circle_params[idx]\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cdnet(model, optimizer, criterion, device, dataloader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    tbar = tqdm(dataloader)\n",
    "    num_samples = len(dataloader)\n",
    "    for i, sample in enumerate(tbar):\n",
    "        image, target = sample[0].float(), sample[1].float()\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        tbar.set_description('Train loss:  %.3f' % (train_loss / (i + 1)))\n",
    "    return train_loss\n",
    "\n",
    "def validate_cdnet(model, criterion, device, dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    tbar = tqdm(dataloader)\n",
    "    num_samples = len(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(tbar):\n",
    "            image, target = sample[0].float(), sample[1].float()\n",
    "            image, target = image.to(device), target.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            tbar.set_description('Val loss:    %.3f' % (train_loss / (i + 1)))\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def test_cdnet(model, device, dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    tbar = tqdm(dataloader)\n",
    "    num_samples = len(dataloader)\n",
    "    outputs = []\n",
    "    ious = []\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(tbar):\n",
    "\n",
    "            image = sample[0].float()\n",
    "            image = image.to(device)\n",
    "            outputs.append([sample[0], model(image), sample[1]])\n",
    "\n",
    "    for bdx, b in enumerate(outputs):\n",
    "        for idx , i in enumerate(zip(b[0], b[1], b[2])):\n",
    "            img = i[0].cpu().numpy()\n",
    "            pred_params = i[1].cpu().numpy()\n",
    "            pred_params = [\n",
    "                pred_params[0]*100+100,\n",
    "                pred_params[1]*100+100,\n",
    "                pred_params[2]*40+10\n",
    "\n",
    "            ]\n",
    "            target_params = i[2].cpu().numpy()\n",
    "            target_params = [\n",
    "                target_params[0]*100+100,\n",
    "                target_params[1]*100+100,\n",
    "                target_params[2]*40+10\n",
    "\n",
    "            ]\n",
    "\n",
    "            ious.append(cvp_utils.iou(target_params, pred_params))\n",
    "    ious = np.asarray(ious)\n",
    "    return np.mean(ious > 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__N.B.__ While training DnCNN, the ride through the cost valley was smooth and mostly linear, however in case of CDNet the optimizer required a lot of navigating and guiding in form of model restarts and changing learning rate. As a result loading the DnCNN backbone weights using `CDNet._init_bbone()` should be carried out only once while the first training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = CIRCLEDataset(\n",
    "    count=10000,\n",
    "    random_noise=False,\n",
    "    noise=2,\n",
    "    debug=False\n",
    ")\n",
    "val_dataset = CIRCLEDataset(\n",
    "    count=1000,\n",
    "    noise=2,\n",
    "    random_noise=False,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "test_dataset = CIRCLEDataset(\n",
    "    count=1000,\n",
    "    noise=2,\n",
    "    random_noise=False,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "train_dataloader = torch_data.DataLoader(train_dataset, num_workers=0, batch_size=32)\n",
    "val_dataloader = torch_data.DataLoader(val_dataset, num_workers=0, batch_size=32)\n",
    "test_dataloader = torch_data.DataLoader(test_dataset, num_workers=0, batch_size=32)\n",
    "\n",
    "\n",
    "model = CDNet(\n",
    "    in_planes=1,\n",
    "    bbone=DnCNN(),\n",
    ")\n",
    "\n",
    "# use this only for the first run\n",
    "model._init_bbone('./results/models/dncnn-70.pth')\n",
    "model.to(device)\n",
    "\n",
    "print(\"total parameters: {}\".format(total_parameters(model)))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    lr=0.005,\n",
    "    weight_decay=1e-3,\n",
    "    params=filter(lambda p: p.requires_grad, model.parameters())\n",
    ")\n",
    "# uncomment this if the training is being \n",
    "# restarted from a local minimum.\n",
    "\"\"\"\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 0.000005\n",
    "\"\"\"\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.5, verbose=True\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_meta = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, optimizer, criterion, device, train_dataloader)\n",
    "    val_loss = validate(model, criterion, device, val_dataloader)\n",
    "    test_score = test(model, device, test_dataloader)\n",
    "\n",
    "    scheduler.step(test_score)\n",
    "\n",
    "    print(epoch, train_loss, val_loss, test_score)\n",
    "\n",
    "    train_meta.append(\n",
    "        [train_loss, val_loss, test_score]\n",
    "    )\n",
    "\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    model_save_str = './results/models/{}-{}.{}'.format(\n",
    "        model.name, epoch, \"pth\"\n",
    "    )\n",
    "\n",
    "    torch.save(state, model_save_str)\n",
    "    np.save(\"train_meta_param\", np.array(train_meta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
